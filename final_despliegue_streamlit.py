# -*- coding: utf-8 -*-
"""FINAL_DESPLIEGUE_STREAMLIT

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1x-C1wVk-hvJyPaGpYXon3r-ATVDpH8mC
"""

# prompt: Realiza todo el despliegue anterior con streamlit y agrega la opción para que el usuario pueda subir su propio data set para realizar predicciones.

import streamlit as st
import pandas as pd
import numpy as np
import joblib
import os

# Function to extract time
def extraer_hora(valor):
    try:
        if isinstance(valor, str):
            valor = valor.replace('a. m.', 'AM').replace('p. m.', 'PM')
            return pd.to_datetime(valor, errors='coerce').time()
        elif isinstance(valor, pd.Timestamp):
            return valor.time()
    except:
        return np.nan

# Function to classify time slot
def clasificar_franja(hora):
    if pd.isnull(hora):
        return np.nan
    elif hora >= pd.to_datetime('00:00').time() and hora < pd.to_datetime('06:00').time():
        return 'MADRUGADA'
    elif hora >= pd.to_datetime('06:00').time() and hora < pd.to_datetime('12:00').time():
        return 'MAÑANA'
    elif hora >= pd.to_datetime('12:00').time() and hora < pd.to_datetime('18:00').time():
        return 'TARDE'
    else:
        return 'NOCHE'

# Function to classify neighborhood zones
def zonas_barrios(BARRIO):
    ZONA_1 = ['LAS VEGAS']
    ZONA_2 = ['EL PORTAL', 'SAN MARCOS', 'PONTEVEDRA', 'JARDINES', 'VILLAGRANDE', 'BOSQUES DE ZUÑIGA']
    ZONA_3 = ['LA SEBASTIANA', 'LAS FLORES', 'URIBE ANGEL', 'ALTO DE MISAEL', 'LAS ORQUIDEAS']
    ZONA_4 = ['EL ESMERALDAL', 'LOMA DEL ATRAVEZADO', 'ZUÑIGA']
    ZONA_5 = ['LOMA DE LAS BRUJAS', 'LA PRADERA', 'EL CHOCHO', 'LA INMACULADA']
    ZONA_6 = ['EL CHINGUI', 'EL SALADO', 'LA MINA', 'SAN RAFAEL', 'SAN JOSE']
    ZONA_7 = ['LAS ANTILLAS', 'EL TRIANON', 'LOMA DEL BARRO', 'LA PAZ', 'EL DORADO']
    ZONA_8 = ['LAS CASITAS', 'PRIMAVERA', 'MILAN VALLEJUELOS', 'ALCALA']
    ZONA_9 = ['LOS NARANJOS', 'MESA', 'ZONA CENTRO', 'OBRERO', 'BUCAREST', 'LA MAGNOLIA']
    VEREDAS = ['VEREDA EL ESCOBERO', 'VEREDA SANTA CATALINA', 'VEREDA EL VALLANO', 'VEREDA LAS PALMAS', 'VEREDA PENTANILLO', 'VEREDA PERICO']

    if BARRIO in (ZONA_1 + ZONA_2 + ZONA_8):
        return 'ZONA 1'
    elif BARRIO in (ZONA_9 + ZONA_7 + ZONA_6):
        return 'ZONA 2'
    elif BARRIO in (ZONA_3 + ZONA_4 + ZONA_5):
        return 'ZONA 3'
    elif BARRIO in VEREDAS:
        return 'VEREDA'
    else:
        return np.nan

# Function to load mappings
def cargar_mapping(nombre_archivo):
    mapping = {}
    try:
        with open(nombre_archivo, 'r', encoding='utf-8') as f:
            for line in f:
                if line.startswith("Mapping for"):
                    continue
                if '->' in line:
                    clave, valor = line.strip().split('->')
                    mapping[clave.strip()] = int(valor.strip())
    except FileNotFoundError:
        st.error(f"Error: Archivo de mapeo no encontrado: {nombre_archivo}")
        return None
    return mapping

# Load saved objects
try:
    one_hot_encoder = joblib.load('one_hot_encoder.joblib')
    model = joblib.load('best_model.joblib')
    label_encoder_gravedad = joblib.load('label_encoder_GRAVEDAD.joblib')
    DIA_SEMANA_mapping = cargar_mapping('DIA_SEMANA_mapping.txt')
    HORA_mapping = cargar_mapping('HORA_mapping.txt')
except FileNotFoundError as e:
    st.error(f"Error loading model components: {e}. Please ensure all necessary files (one_hot_encoder.joblib, best_model.joblib, label_encoder_GRAVEDAD.joblib, DIA_SEMANA_mapping.txt, HORA_mapping.txt) are in the correct directory.")
    st.stop() # Stop the app if files are not found

# --- Streamlit App ---
st.title('Aplicación de Predicción de Gravedad de Accidentes de Tránsito')
st.write('Esta aplicación predice la gravedad de un accidente de tránsito basado en sus características.')

# Option for user to upload their own dataset
uploaded_file = st.file_uploader("Sube tu propio archivo CSV o Excel para realizar predicciones (Opcional)", type=["csv", "xlsx", "xls"])

if uploaded_file is not None:
    st.write("Cargando archivo subido por el usuario...")
    try:
        if uploaded_file.name.endswith('.csv'):
            df_input = pd.read_csv(uploaded_file)
        else:
            df_input = pd.read_excel(uploaded_file)

        st.write("Archivo cargado exitosamente.")
        st.dataframe(df_input.head())

        # --- Data Preprocessing for Uploaded Data ---
        st.write("Preprocesando datos...")

        # Select relevant columns
        columns_to_keep = ['HORA', 'DÍA DE LA SEMANA', 'RESULTADO DE BEODEZ', 'CLASE DE ACCIDENTE', 'BARRIO']
        # Check if all required columns are present
        if not all(col in df_input.columns for col in columns_to_keep):
            missing_cols = [col for col in columns_to_keep if col not in df_input.columns]
            st.error(f"El archivo CSV subido debe contener las siguientes columnas: {', '.join(columns_to_keep)}. Faltan: {', '.join(missing_cols)}")
            st.stop() # Stop if required columns are missing

        df_processed_user = df_input[columns_to_keep].copy() # Use .copy() to avoid SettingWithCopyWarning

        # Apply preprocessing functions
        df_processed_user["HORA"] = df_processed_user["HORA"].apply(extraer_hora)
        df_processed_user["HORA"] = df_processed_user["HORA"].apply(clasificar_franja)
        df_processed_user['BARRIO'] = df_processed_user['BARRIO'].apply(zonas_barrios)

        # Validate and clean 'CLASE DE ACCIDENTE'
        valores_permitidos = ['ATROPELLO', 'CAIDA OCUPANTE', 'CHOQUE', 'INCENDIO', 'VOLCAMIENTO']
        df_processed_user['CLASE DE ACCIDENTE'] = df_processed_user['CLASE DE ACCIDENTE'].apply(lambda x: x if x in valores_permitidos else np.nan)

        # Validate and clean 'DÍA DE LA SEMANA'
        valores_permitidos_dias = ['Domingo', 'Jueves', 'Lunes', 'Martes', 'Miércoles', 'Sábado', 'Viernes']
        df_processed_user['DÍA DE LA SEMANA'] = df_processed_user['DÍA DE LA SEMANA'].apply(lambda x: x if x in valores_permitidos_dias else np.nan)

        # Validate and clean 'RESULTADO DE BEODEZ'
        df_processed_user['RESULTADO DE BEODEZ'] = df_processed_user['RESULTADO DE BEODEZ'].astype(str).str.strip()
        df_processed_user['RESULTADO DE BEODEZ'] = pd.to_numeric(df_processed_user['RESULTADO DE BEODEZ'], errors='coerce')
        df_processed_user['RESULTADO DE BEODEZ'] = df_processed_user['RESULTADO DE BEODEZ'].where(df_processed_user['RESULTADO DE BEODEZ'].isin([0, 1, 2, 3]), np.nan)

        # Handle missing values
        initial_rows = len(df_processed_user)
        df_processed_user.dropna(inplace=True)
        rows_dropped = initial_rows - len(df_processed_user)
        if rows_dropped > 0:
            st.warning(f"Se eliminaron {rows_dropped} filas debido a valores faltantes después del preprocesamiento.")


        # Apply mappings (check if mappings were loaded successfully)
        if DIA_SEMANA_mapping is not None and HORA_mapping is not None:
            df_processed_user['DÍA DE LA SEMANA'] = df_processed_user['DÍA DE LA SEMANA'].map(DIA_SEMANA_mapping)
            df_processed_user['HORA'] = df_processed_user['HORA'].map(HORA_mapping)
            # Drop rows where mapping resulted in NaN (if any values were not in the mapping)
            initial_rows_after_dropna = len(df_processed_user)
            df_processed_user.dropna(subset=['DÍA DE LA SEMANA', 'HORA'], inplace=True)
            rows_dropped_mapping = initial_rows_after_dropna - len(df_processed_user)
            if rows_dropped_mapping > 0:
                st.warning(f"Se eliminaron {rows_dropped_mapping} filas donde los valores de 'DÍA DE LA SEMANA' o 'HORA' no pudieron ser mapeados.")

        else:
             st.error("No se pudieron cargar los archivos de mapeo. No se puede continuar con el preprocesamiento.")
             st.stop()


        # Apply One-Hot Encoding
        columns_to_encode = ['CLASE DE ACCIDENTE', 'BARRIO']
        try:
            encoded_data = one_hot_encoder.transform(df_processed_user[columns_to_encode])
            encoded_df = pd.DataFrame(encoded_data, columns=one_hot_encoder.get_feature_names_out(columns_to_encode))
        except ValueError as e:
            st.error(f"Error durante el One-Hot Encoding. Esto podría deberse a categorías en 'CLASE DE ACCIDENTE' o 'BARRIO' en tu archivo que no fueron vistas durante el entrenamiento del modelo: {e}")
            st.stop()


        df_processed_user = df_processed_user.drop(columns=columns_to_encode)
        df_processed_user.rename(columns={'HORA': 'HORA_CODIFICADA', 'DÍA DE LA SEMANA': 'DIA_SEMANA_CODIFICADO'}, inplace=True)

        df_processed_user.reset_index(drop=True, inplace=True)
        encoded_df.reset_index(drop=True, inplace=True)
        df_processed_user = pd.concat([df_processed_user, encoded_df], axis=1)

        # Reorder columns
        column_order = [
            'RESULTADO DE BEODEZ',
            'DIA_SEMANA_CODIFICADO',
            'HORA_CODIFICADA',
            'CLASE DE ACCIDENTE_ATROPELLO',
            'CLASE DE ACCIDENTE_CAIDA OCUPANTE',
            'CLASE DE ACCIDENTE_CHOQUE',
            'CLASE DE ACCIDENTE_INCENDIO',
            'CLASE DE ACCIDENTE_VOLCAMIENTO',
            'BARRIO_VEREDA',
            'BARRIO_ZONA 1',
            'BARRIO_ZONA 2',
            'BARRIO_ZONA 3'
        ]

        # Ensure all required columns are in the processed dataframe before reordering
        missing_cols_for_prediction = [col for col in column_order if col not in df_processed_user.columns]
        if missing_cols_for_prediction:
            st.error(f"Después del preprocesamiento, faltan columnas necesarias para la predicción: {', '.join(missing_cols_for_prediction)}. Esto puede deberse a categorías de barrio o tipo de accidente que no existen en el conjunto de entrenamiento original.")
            st.stop()


        df_processed_user = df_processed_user[column_order]

        st.write("Datos preprocesados listos para predicción.")
        st.dataframe(df_processed_user.head())


        # --- Make Predictions on Uploaded Data ---
        st.write("Realizando predicciones...")
        predictions_user = model.predict(df_processed_user)
        df_processed_user['prediction'] = predictions_user.astype(int)
        df_processed_user['predicted_GRAVEDAD'] = label_encoder_gravedad.inverse_transform(df_processed_user['prediction'])

        st.write("Predicciones completadas.")
        st.dataframe(df_processed_user[['prediction', 'predicted_GRAVEDAD']].head())

        st.write("Distribución de las predicciones:")
        st.write(df_processed_user['predicted_GRAVEDAD'].value_counts())

        # Option to download predictions
        @st.cache_data
        def convert_df_to_csv(df):
            # IMPORTANT: This allows the cache mechanism to track changes in the df
            return df.to_csv(index=False).encode('utf-8')

        csv_data = convert_df_to_csv(df_processed_user[['prediction', 'predicted_GRAVEDAD']])
        st.download_button(
            label="Descargar predicciones como CSV",
            data=csv_data,
            file_name='predicciones_accidentes.csv',
            mime='text/csv',
        )


    except Exception as e:
        st.error(f"Ocurrió un error al procesar el archivo subido: {e}")

else:
    st.info("Sube un archivo CSV para obtener predicciones o usa las opciones de la barra lateral para predicciones individuales.")

# --- Individual Prediction (Optional) ---
st.sidebar.header('Predicción Individual')
st.sidebar.write("Selecciona los parámetros del accidente para predecir su gravedad.")

dia_semana = st.sidebar.selectbox('Día de la Semana', ('Lunes', 'Martes', 'Miércoles', 'Jueves', 'Viernes', 'Sábado', 'Domingo'))
hora_dia = st.sidebar.selectbox('Franja Horaria', ('MADRUGADA', 'MAÑANA', 'TARDE', 'NOCHE'))
beodez = st.sidebar.selectbox('Resultado de Beodez', (0, 1, 2, 3))
clase_accidente = st.sidebar.selectbox('Clase de Accidente', ('ATROPELLO', 'CAIDA OCUPANTE', 'CHOQUE', 'INCENDIO', 'VOLCAMIENTO'))
barrio = st.sidebar.selectbox('Zona/Vereda del Barrio', ('VEREDA', 'ZONA 1', 'ZONA 2', 'ZONA 3'))

if st.sidebar.button('Predecir Gravedad'):
    if DIA_SEMANA_mapping is None or HORA_mapping is None:
        st.sidebar.error("No se pudieron cargar los archivos de mapeo. No se puede realizar la predicción individual.")
    else:
        # Create a dataframe for the individual prediction
        data = {
            'DÍA DE LA SEMANA': [dia_semana],
            'HORA': [hora_dia],
            'RESULTADO DE BEODEZ': [beodez],
            'CLASE DE ACCIDENTE': [clase_accidente],
            'BARRIO': [barrio]
        }
        df_individual = pd.DataFrame(data)

        # Preprocess the individual input
        df_individual['DÍA DE LA SEMANA'] = df_individual['DÍA DE LA SEMANA'].map(DIA_SEMANA_mapping)
        df_individual['HORA'] = df_individual['HORA'].map(HORA_mapping)

        # Apply One-Hot Encoding for the individual input
        columns_to_encode_individual = ['CLASE DE ACCIDENTE', 'BARRIO']
        try:
            encoded_data_individual = one_hot_encoder.transform(df_individual[columns_to_encode_individual])
            encoded_df_individual = pd.DataFrame(encoded_data_individual, columns=one_hot_encoder.get_feature_names_out(columns_to_encode_individual))
        except ValueError as e:
             st.sidebar.error(f"Error durante el One-Hot Encoding para la entrada individual: {e}")
             encoded_df_individual = pd.DataFrame(np.zeros((len(df_individual), len(one_hot_encoder.get_feature_names_out(columns_to_encode_individual)))),
                                                  columns=one_hot_encoder.get_feature_names_out(columns_to_encode_individual))
             st.sidebar.warning("Se procedió con valores por defecto para el encoding. La predicción podría no ser precisa.")


        df_individual = df_individual.drop(columns=columns_to_encode_individual)
        df_individual.rename(columns={'HORA': 'HORA_CODIFICADA', 'DÍA DE LA SEMANA': 'DIA_SEMANA_CODIFICADO'}, inplace=True)

        df_individual.reset_index(drop=True, inplace=True)
        encoded_df_individual.reset_index(drop=True, inplace=True)
        df_individual_processed = pd.concat([df_individual, encoded_df_individual], axis=1)

        # Reorder columns to match model's expected input
        column_order_individual = [col for col in column_order if col != 'prediction'] # Remove 'prediction' from the order

        # Ensure all columns required by the model are present, add if missing (with 0)
        for col in column_order_individual:
            if col not in df_individual_processed.columns:
                df_individual_processed[col] = 0

        df_individual_processed = df_individual_processed[column_order_individual]


        # Make prediction
        prediction_individual = model.predict(df_individual_processed)[0]
        predicted_gravedad_individual = label_encoder_gravedad.inverse_transform([prediction_individual])[0]

        st.sidebar.subheader('Resultado de la Predicción Individual:')
        st.sidebar.write(f'La gravedad predicha del accidente es: **{predicted_gravedad_individual}**')


# To run this app:
# 1. Save the code as a Python file (e.g., app.py)
# 2. Make sure 'one_hot_encoder.joblib', 'best_model.joblib', 'label_encoder_GRAVEDAD.joblib',
#    'DIA_SEMANA_mapping.txt', and 'HORA_mapping.txt' are in the same directory.
# 3. Open your terminal or Anaconda Prompt.
# 4. Navigate to the directory where you saved the file.
# 5. Run the command: streamlit run app.py
# 6. A new tab will open in your web browser with the Streamlit app.